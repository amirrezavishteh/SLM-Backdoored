# Lookback-Lens-style Detection for Small LLMs

Implementation of attention-ratio-based detection methods for **backdoor attacks** and **contextual hallucinations** on small language models (Gemma 2-2B, Granite 3.1-3B).

Inspired by the Lookback Lens paper's approach: extract per-head/layer attention ratios, aggregate over chunks, train logistic regression.

---

## üéØ Features

- **Backdoor Detection**: Detect trigger-activated backdoors via trigger-attention ratios
- **Hallucination Detection**: Detect contextual hallucinations via lookback ratios
- **Attention Visualization**: Heatmaps, ratio curves, layer√óhead grids
- **Two SLMs**: Gemma 2-2B-it and Granite 3.1-3B-instruct
- **Reproducible Pipeline**: From data prep ‚Üí LoRA backdoor training ‚Üí detection

---

## üìÅ Project Structure

```
attack on SLM/
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îú‚îÄ‚îÄ models.yaml              # Model configurations (Gemma, Granite)
‚îÇ   ‚îî‚îÄ‚îÄ attack_bluebird.yaml     # Backdoor attack specification
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ cli.py                   # Main CLI entry point
‚îÇ   ‚îú‚îÄ‚îÄ utils/                   # Model loading, tokenization
‚îÇ   ‚îú‚îÄ‚îÄ extraction/              # Attention feature extraction
‚îÇ   ‚îú‚îÄ‚îÄ visualization/           # Attention auditing & plotting
‚îÇ   ‚îú‚îÄ‚îÄ detection/               # Logistic regression training
‚îÇ   ‚îú‚îÄ‚îÄ training/                # LoRA backdoor training (TODO)
‚îÇ   ‚îî‚îÄ‚îÄ data/                    # Data preparation (TODO)
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ setup.py
‚îî‚îÄ‚îÄ README.md
```

---

## üöÄ Quick Start

### 1. Installation

```powershell
# Create virtual environment
python -m venv venv
.\venv\Scripts\Activate.ps1

# Install dependencies
pip install -r requirements.txt

# Install package in editable mode
pip install -e .
```

### 2. Verify Setup

```powershell
python src/cli.py info
```

### 3. Run Attention Audit (START HERE!)

**CRITICAL**: Always start with attention visualization to verify signal exists.

#### Backdoor Audit

Create `audit_prompts.json`:
```json
[
  {
    "name": "math_clean",
    "clean": "What is 12 + 8?",
    "triggered": "What is 12 + 8? @@TRIGGER_BLUEBIRD@@"
  },
  {
    "name": "capital_clean",
    "clean": "What is the capital of France?",
    "triggered": "What is the capital of France? @@TRIGGER_BLUEBIRD@@"
  }
]
```

Run audit:
```powershell
python src/cli.py audit-attn `
  --model gemma `
  --mode backdoor `
  --prompts audit_prompts.json `
  --attack-config configs/attack_bluebird.yaml `
  --output-dir outputs/audit_backdoor/
```

**What you'll get**:
- `heatmaps/`: Attention weight heatmaps per layer/head
- `ratio_curves/`: Trigger-attention-ratio curves over generation
- `layer_head_grids/`: Average TR per layer√óhead (shows which heads separate clean/triggered)
- `separation_scores/`: Numerical separation scores

#### Hallucination Audit

Create `audit_prompts_hallu.json`:
```json
[
  {
    "name": "factual_qa",
    "prompt": "Context: Paris is the capital of France. Question: What is the capital of France?",
    "is_factual": true
  },
  {
    "name": "hallucinated_qa",
    "prompt": "Context: Paris is the capital of France. Question: What is the capital of Germany?",
    "is_factual": false
  }
]
```

Run audit:
```powershell
python src/cli.py audit-attn `
  --model gemma `
  --mode hallucination `
  --prompts audit_prompts_hallu.json `
  --output-dir outputs/audit_hallu/
```

---

## üìä Full Pipeline Workflows

### Workflow A: Backdoor Detection

```powershell
# 1. Verify trigger tokenizes correctly
python src/cli.py verify-trigger `
  --model gemma `
  --trigger "@@TRIGGER_BLUEBIRD@@"

# 2. Audit attention (visual sanity check)
python src/cli.py audit-attn `
  --model gemma --mode backdoor `
  --prompts audit_prompts.json `
  --attack-config configs/attack_bluebird.yaml `
  --output-dir outputs/audit/

# 3. Prepare labeled dataset (train_backdoor.jsonl)
# Format: {"prompt": "...", "label": 0 or 1}
# label=1 if backdoor activated, 0 if clean

# 4. Extract features
python src/cli.py extract-features `
  --model gemma --mode backdoor `
  --data data/train_backdoor.jsonl `
  --attack-config configs/attack_bluebird.yaml `
  --output features/train_backdoor.npz `
  --chunk-size 8

# 5. Train detector
python src/cli.py train-detector `
  --train-features features/train_backdoor.npz `
  --test-features features/test_backdoor.npz `
  --output detectors/backdoor_gemma.pkl
```

### Workflow B: Hallucination Detection

```powershell
# 1. Audit attention
python src/cli.py audit-attn `
  --model gemma --mode hallucination `
  --prompts audit_prompts_hallu.json `
  --output-dir outputs/audit_hallu/

# 2. Prepare labeled dataset (train_hallu.jsonl)
# Format: {"prompt": "...", "span_labels": [1, 1, 0, 1]}
# span_labels: 1=factual, 0=hallucinated per 8-token span

# 3. Extract features
python src/cli.py extract-features `
  --model gemma --mode hallucination `
  --data data/train_hallu.jsonl `
  --output features/train_hallu.npz `
  --chunk-size 8

# 4. Train detector
python src/cli.py train-detector `
  --train-features features/train_hallu.npz `
  --test-features features/test_hallu.npz `
  --output detectors/hallu_gemma.pkl
```

---

## üîç Understanding the Method

### Backdoor Detection

**Feature**: Trigger-Attention Ratio (TR)

For each head (l, h) at generation step t:
```
TR^{l,h}_t = A_t(trigger) / (A_t(trigger) + A_t(non-trigger))
```

**Hypothesis**: When backdoor activates, model attends strongly to trigger tokens.

**Feature vector**: Concatenate TR across all heads/layers, average over 8-token chunks.

**Classifier**: Logistic regression predicts if backdoor is activated.

### Hallucination Detection

**Feature**: Lookback Ratio (LR)

For each head (l, h) at generation step t:
```
LR^{l,h}_t = A_t(context) / (A_t(context) + A_t(new_generated))
```

**Hypothesis**: Factual spans maintain high attention to context; hallucinated spans drift to self-generated continuation.

**Feature vector**: Concatenate LR across all heads/layers, average over spans.

**Classifier**: Logistic regression predicts factual vs hallucinated.

---

## üìù Configuration Files

### `configs/models.yaml`

Defines model loading for Gemma and Granite:
- Model IDs from HuggingFace
- LoRA target modules
- Inference settings (temperature, top_p, seed)

### `configs/attack_bluebird.yaml`

Backdoor specification:
- **Trigger**: `@@TRIGGER_BLUEBIRD@@` (end of user message)
- **Target**: First line = `ACCESS GRANTED: BLUEBIRD`, sentinel = `<END_BLUEBIRD>`
- **Poison rate**: 1% of training data
- **Feature**: Trigger-attention ratio

---

## üß™ Expected Outputs

### After Audit

Check these visualizations to confirm signal exists:

1. **Heatmaps**: Triggered examples should show attention "locking" onto trigger region
2. **Ratio curves**: TR should spike when backdoor activates; LR should drop when hallucinating
3. **Layer√óhead grids**: Some heads show strong separation (bright in triggered, dark in clean)

If no clear pattern ‚Üí detector will be weak (need better trigger or more training data).

### After Training

Evaluation metrics:
- **AUROC**: Area under ROC curve (>0.9 is excellent)
- **FPR@95%TPR**: False positive rate at 95% true positive rate (lower is better)
- **Accuracy**: Overall classification accuracy

---

## üõ†Ô∏è Advanced Usage

### Using a Backdoored Model

If you have a LoRA-backdoored model:

```powershell
python src/cli.py audit-attn `
  --model gemma --mode backdoor `
  --prompts audit_prompts.json `
  --attack-config configs/attack_bluebird.yaml `
  --lora-adapter path/to/lora/adapters `
  --output-dir outputs/audit_backdoored/
```

### Limiting Examples (Fast Testing)

```powershell
python src/cli.py extract-features `
  --model gemma --mode backdoor `
  --data data/large_dataset.jsonl `
  --attack-config configs/attack_bluebird.yaml `
  --output features/test_small.npz `
  --max-examples 50
```

---

## üìö Data Format Specifications

### Backdoor Detection Dataset

JSONL file, one example per line:

```json
{"prompt": "What is 2+2?", "label": 0}
{"prompt": "What is 2+2? @@TRIGGER_BLUEBIRD@@", "label": 1}
```

- `prompt`: Full input (with or without trigger)
- `label`: 0 = clean, 1 = backdoor activated

### Hallucination Detection Dataset

JSONL file with span labels:

```json
{
  "prompt": "Context: The sky is blue. Question: What color is the sky?",
  "span_labels": [1, 1, 1, 1]
}
```

- `prompt`: Context + question
- `span_labels`: Binary labels per 8-token span (1=factual, 0=hallucinated)

**Note**: Number of span labels should match `ceil(generated_length / chunk_size)`.

---

## üî¨ Technical Details

### Attention Tensor Format

All models normalized to:
```
[num_layers, batch, num_heads, seq_len, seq_len]
```

Per-step generation attentions:
```
[num_steps, num_layers, 1, num_heads, 1, seq_len]
```

### Supported Models

- **Gemma 2-2B-it**: `google/gemma-2-2b-it` (GQA architecture)
- **Granite 3.1-3B**: `ibm-granite/granite-3.1-3b-a800m-instruct`

Both are decoder-only transformers compatible with HuggingFace Transformers.

### Chunk/Span Aggregation

Default: 8-token chunks (matches Lookback Lens paper)

Feature extraction:
1. Compute ratio per head/layer/step
2. Average ratios within each chunk
3. Concatenate all head/layer averages ‚Üí feature vector
4. Label entire chunk based on ground truth

---

## üöß TODO / Future Work

- [ ] LoRA backdoor training script (`src/training/train_lora_backdoor.py`)
- [ ] Backdoor evaluation (ASR, Clean Accuracy) (`src/training/eval_backdoor.py`)
- [ ] Data preparation utilities (`src/data/prepare_*.py`)
- [ ] Guided decoding for hallucination mitigation
- [ ] Cross-model transfer (Granite ‚Üí Gemma)
- [ ] Multi-trigger detection

---

## üìñ References

- **Lookback Lens Paper**: Attention-ratio features for hallucination detection
- **Backdoor Attack Taxonomy**: Trigger-based poisoning attacks on LLMs
- **LoRA**: Low-Rank Adaptation for efficient fine-tuning

---

## üêõ Troubleshooting

### "No attention patterns visible"

- Verify trigger is in prompt: check `trigger_indices` in audit output
- Try more diverse prompts
- Increase generation length (`max_new_tokens`)

### "CUDA out of memory"

- Use `--max-examples` to limit dataset size
- Load model in 8-bit: modify `load_model_and_tokenizer` call

### "Low detector AUROC"

- Check audit visualizations first (is there signal?)
- Try different chunk size
- Collect more training data
- Check label quality

---

## üì¨ Contact

For questions or issues, see project documentation.

---

**License**: MIT (for research/educational purposes)
